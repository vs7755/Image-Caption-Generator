{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, BlipConfig\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and processor\n",
    "MODEL_DIRECTORY = \"blip-image-captioning-base\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIRECTORY)\n",
    "PROCESSOR_PATH = os.path.join(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(PROCESSOR_PATH)\n",
    "config = BlipConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "# Define a custom model architecture with two additional layers\n",
    "class CustomBlipForConditionalGeneration(BlipForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Define additional layers\n",
    "        self.additional_layer1 = nn.Linear(768, 768)  # Adjust the size as needed\n",
    "        self.additional_layer2 = nn.Linear(768, 768)  # Adjust the size as needed\n",
    "        # Initialize additional layers\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Apply additional layers\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.additional_layer1(sequence_output)\n",
    "        sequence_output = nn.functional.relu(sequence_output)\n",
    "        sequence_output = self.additional_layer2(sequence_output)\n",
    "        return sequence_output, outputs[1:]\n",
    "\n",
    "\n",
    "model = CustomBlipForConditionalGeneration.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Flicker8k_Dataset\\\\10815824_2997e03d76.jpg\"\n",
    "# Display the uploaded image on the sidebar\n",
    "image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Function to generate unconditional caption\n",
    "def generate_caption(image):\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_references(file_path):\n",
    "    references = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            image_id = line.split()[0][:-2]\n",
    "            caption = line.split()[1:]\n",
    "            caption = ' '.join(caption)\n",
    "            if image_id not in references:\n",
    "                references[image_id] = []\n",
    "            references[image_id].append(caption)\n",
    "    return references\n",
    "\n",
    "references = load_references('Flickr8k.token.txt')\n",
    "\n",
    "# Generate captions for each image in the dataset\n",
    "generated_captions = {}\n",
    "for image_id in tqdm(references.keys()):\n",
    "    file_path = f\"Flicker8k_Dataset\\\\{image_id}\"\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    generated_caption = generate_caption(image)\n",
    "    generated_captions[image_id] = generated_caption\n",
    "\n",
    "# Prepare references and hypotheses for computing BLEU score\n",
    "references_list = [\n",
    "    [caption.split() for caption in captions] for captions in references.values()\n",
    "]\n",
    "hypotheses_list = [\n",
    "    generated_captions[image_id].split() for image_id in references.keys()\n",
    "]\n",
    "# Compute BLEU score\n",
    "bleu_score = corpus_bleu(references_list, hypotheses_list)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:37<00:00,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.29443704667018933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_references(file_path):\n",
    "    references = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            image_id = line.split()[0][:-2]\n",
    "            caption = line.split()[1:]\n",
    "            caption = ' '.join(caption)\n",
    "            if image_id not in references:\n",
    "                references[image_id] = []\n",
    "            references[image_id].append(caption)\n",
    "    return references\n",
    "\n",
    "references = load_references('Flickr8k.token.test.txt')\n",
    "\n",
    "# Generate captions for each image in the dataset\n",
    "generated_captions = {}\n",
    "for image_id in tqdm(references.keys()):\n",
    "    file_path = f\"Flicker8k_Dataset_test\\\\{image_id}\"\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    generated_caption = generate_caption(image)\n",
    "    generated_captions[image_id] = generated_caption\n",
    "\n",
    "# Prepare references and hypotheses for computing BLEU score\n",
    "references_list = [\n",
    "    [caption.split() for caption in captions] for captions in references.values()\n",
    "]\n",
    "hypotheses_list = [\n",
    "    generated_captions[image_id].split() for image_id in references.keys()\n",
    "]\n",
    "# Compute BLEU score\n",
    "bleu_score = corpus_bleu(references_list, hypotheses_list)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
